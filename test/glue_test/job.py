# UNIT TEST FOR GLUEJOBUTILS
import sys
import os

# from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

from gluejobutils import s3, utils, scd, datatypes

import datetime
from pyspark.sql import Row, functions as F
from pyspark.sql.types import IntegerType

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'metadata_base_path'])

sc = SparkContext()

glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

print(sys.version)


### ### ### ### ### ### ### ### 
### dea_record_start_date ###
### ### ### ### ### ### ### ### 
csv_path = 's3://alpha-gluejobutils/testing/data/diamonds_csv/'
meta_path = 's3://alpha-gluejobutils/testing/meta_data/diamonds.json'
start_date = '2018-01-01'
start_datetime = '2018-01-01 01:00:00'
meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df = spark.read.csv(csv_path, header = True, schema = meta)
df = scd.set_dea_record_start_datetime(df, '2018-01-01 01:00:00')
df.write.partitionBy('dea_record_start_datetime').mode('overwrite').format('parquet').save('s3://alpha-gluejobutils/silly_folder4/')

## =====================> INIT TEST MODULE TESTING <========================= ##
s3.delete_s3_folder_contents('s3://alpha-gluejobutils/testing/data_dump/')
s3.delete_s3_folder_contents('s3://alpha-gluejobutils/database/')

csv_path = 's3://alpha-gluejobutils/testing/data/diamonds_csv/'
meta_path = 's3://alpha-gluejobutils/testing/meta_data/diamonds.json'

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df_old = spark.read.csv(csv_path, header = True, schema=meta)
df_old = scd.init_dea_record_datetimes(df_old, '2018-01-01 01:00:00')
df_old.write.mode('overwrite').parquet('s3://alpha-gluejobutils/database/table1/')

## =====================> UTILS MODULE TESTING <========================= ##
a = 'test/folder/path/'
b = 'test/folder/path'
if utils.add_slash(a) != a :
    raise ValueError('add_slash FAILURE')
if utils.remove_slash(a) != b :
    raise ValueError('remove_slash FAILURE')
if utils.add_slash(b) != a :
    raise ValueError('add_slash FAILURE')
if utils.remove_slash(b) != b :
    raise ValueError('remove_slash FAILURE')
print("===> utils ===> OK")


## =====================> S3 MODULE TESTING <========================= ##
bucket = 'alpha-gluejobutils'
diamonds_obj = 'testing/data/diamonds.csv'
### ### ### ### ### ### ### ### 
### bucket_key_to_s3_path ### 
### ### ### ### ### ### ### ###
out = s3.bucket_key_to_s3_path(bucket, diamonds_obj)
if out != 's3://alpha-gluejobutils/testing/data/diamonds.csv' :
    raise ValueError('bucket_key_to_s3_path FAILURE')

out = s3.bucket_key_to_s3_path(bucket, 'some/path/')
if out != 's3://alpha-gluejobutils/some/path/' :
    raise ValueError('bucket_key_to_s3_path FAILURE')
    
out = s3.bucket_key_to_s3_path(bucket, 'some/path')
if out != 's3://alpha-gluejobutils/some/path' :
    raise ValueError('bucket_key_to_s3_path FAILURE')

print("===> bucket_key_to_s3_path ===> OK")


### ### ### ### ### ### ### ### 
### s3_path_to_bucket_key ### 
### ### ### ### ### ### ### ### 
b, o = s3.s3_path_to_bucket_key('s3://alpha-gluejobutils/testing/data/diamonds_csv/diamonds.csv')
if b != 'alpha-gluejobutils' or o != 'testing/data/diamonds_csv/diamonds.csv' :
    raise ValueError('s3_path_to_bucket_key FAILURE')

b, o = s3.s3_path_to_bucket_key('s3://alpha-gluejobutils/testing/data')
if b != 'alpha-gluejobutils' or o != 'testing/data' :
    raise ValueError('s3_path_to_bucket_key FAILURE')

b, o = s3.s3_path_to_bucket_key('s3://alpha-gluejobutils/testing/data/')
if b != 'alpha-gluejobutils' or o != 'testing/data/' :
    raise ValueError('s3_path_to_bucket_key FAILURE')
print("===> s3_path_to_bucket_key ===> OK")


### ### ### ### ### ### ###
### read_json_from_s3 ### 
### ### ### ### ### ### ### 
test_json = s3.read_json_from_s3('s3://alpha-gluejobutils/testing/meta_data/diamonds.json')
diff = len(set(['$schema', 'name', 'description', 'data_format', 'columns', 'partitions', 'location']).difference(test_json.keys()))
if diff != 0 :
    raise ValueError('read_json_from_s3 FAILURE')
print("===> read_json_from_s3 ===> OK")


### ### ### ### ### ###
### write_json_to_s3 ### 
### ### ### ### ### ###
json_data = {'a':'dog', 'b':14,'c':[1,2,3],'d':{'cat': 'alpha'}}
s3.write_json_to_s3(json_data, 's3://alpha-gluejobutils/testing/data_dump/test1.json')
json_data2 = s3.read_json_from_s3('s3://alpha-gluejobutils/testing/data_dump/test1.json')

if json_data != json_data2 :
    raise ValueError('read_json_from_s3 FAILURE')
print("===> write_json_to_s3 ===> OK")


### ### ### ### ### ### ###  
### check_for_s3_file ###
### ### ### ### ### ### ### 
if not s3.check_for_s3_file('s3://alpha-gluejobutils/testing/meta_data/diamonds.json') :
    raise ValueError('check_for_s3_file FAILURE')
print("===> check_for_s3_file ===> OK")


### ### ### ### ### ###### ### ### ### 
### get_filepaths_from_s3_folder ### 
### ### ### ### ### ### ### ### ### ### 
out1 = s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing')
out2 = s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/')
for o1, o2 in zip(out1, out2) :
    if o1 != o2 :
        raise ValueError('get_filepaths_from_s3_folder FAILURE')
        
out3 = s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data/', extension='.parquet')
if 'diamonds_parquet' not in out3[0] :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
if len(out3) != 1 :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')

out4 = s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data/diamonds_parquet', extension='parquet')
if 'diamonds_parquet' not in out4[0] :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
if len(out4) != 1 :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
    
out5 = s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data/')
if any([p.endswith('/') or '_SUCCESS' in p for p in out5]) :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
print("===> get_filepaths_from_s3_folder ===> OK")


### ### ### ### ### ###
###Â copy_s3_object ###
### ### ### ### ### ###
s3.copy_s3_object(old_s3_path='s3://alpha-gluejobutils/testing/meta_data/diamonds.json', new_s3_path='s3://alpha-gluejobutils/testing/data_dump/copy_test/file0.json')
s3.copy_s3_object(old_s3_path='s3://alpha-gluejobutils/testing/meta_data/diamonds.json', new_s3_path='s3://alpha-gluejobutils/testing/data_dump/copy_test/file1.json')
json0 = s3.read_json_from_s3('s3://alpha-gluejobutils/testing/data_dump/copy_test/file0.json')
json1 = s3.read_json_from_s3('s3://alpha-gluejobutils/testing/data_dump/copy_test/file1.json')
if json0 != json1 :
    raise ValueError('copy_s3_object FAILURE')
print("===> copy_s3_object ===> OK")


### ### ### ### ### ### ### ### ### ### ### ### 
### copy_s3_folder_contents_to_new_folder ### 
### ### ### ### ### ### ### ### ### ### ### ### 
for i in [1,2,3] :
    s3.copy_s3_object(old_s3_path='s3://alpha-gluejobutils/testing/data_dump/copy_test/file0.json', new_s3_path='s3://alpha-gluejobutils/testing/data_dump/copy_test/file{}.json'.format(i))

s3.copy_s3_folder_contents_to_new_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test/', 's3://alpha-gluejobutils/testing/data_dump/copy_test2/')
for i, p in enumerate(s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test2/')) :
    if p.split('/')[-1] != 'file{}.json'.format(i) :
        raise ValueError('copy_s3_folder_contents_to_new_folder FAILURE')
print("===> copy_s3_folder_contents_to_new_folder ===> OK")


### ### ### ### ### ### ###
### delete_s3_object ###
### ### ### ### ### ### ###
s3.copy_s3_object('s3://alpha-gluejobutils/testing/data_dump/copy_test/file0.json', 's3://alpha-gluejobutils/testing/data_dump/copy_test/copy_file0.json')

s3.delete_s3_object('s3://alpha-gluejobutils/testing/data_dump/copy_test/copy_file0.json')

if s3.check_for_s3_file('s3://alpha-gluejobutils/testing/data_dump/copy_test/copy_file0.json') :
    raise ValueError('delete_s3_object FAILURE')
print("===> delete_s3_object ===> OK")


### ### ### ### ### ### ### ### ###
### delete_s3_folder_contents ###
### ### ### ### ### ### ### ### ###
s3.copy_s3_folder_contents_to_new_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test/', 's3://alpha-gluejobutils/testing/data_dump/copy_test2/')
s3.delete_s3_folder_contents('s3://alpha-gluejobutils/testing/data_dump/copy_test2/')
if len(s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test2/')) != 0 :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
    
s3.copy_s3_folder_contents_to_new_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test', 's3://alpha-gluejobutils/testing/data_dump/copy_test2')
s3.delete_s3_folder_contents('s3://alpha-gluejobutils/testing/data_dump/copy_test2')
if len(s3.get_filepaths_from_s3_folder('s3://alpha-gluejobutils/testing/data_dump/copy_test2')) != 0 :
    raise ValueError('get_filepaths_from_s3_folder FAILURE')
print("===> delete_s3_folder_contents ===> OK")


### ### ### ### ### ### ### ### ### ### ###
### folder_contains_only_files_with_extension ###
### ### ### ### ### ### ### ### ### ### ###
s3_folder_path1 = 's3://alpha-gluejobutils/database/table1/nothing'
s3_folder_path2 = 's3://alpha-gluejobutils/database/table1/'
s3_folder_path3 = 's3://alpha-gluejobutils/testing/data/diamonds_csv/'
s3_folder_path4 = 's3://alpha-gluejobutils/testing/data/diamonds_parquet/'
s3_folder_path5 = 's3://alpha-gluejobutils/testing/data/'

if s3.folder_contains_only_files_with_extension(s3_folder_path1, '.parquet') != False:
    raise ValueError('folder_contains_only_files_with_extension FAILURE')
if s3.folder_contains_only_files_with_extension(s3_folder_path2, 'parquet') != True :
    raise ValueError('folder_contains_only_files_with_extension FAILURE')
if s3.folder_contains_only_files_with_extension(s3_folder_path3, 'csv') != True : 
    raise ValueError('folder_contains_only_files_with_extension FAILURE')
if s3.folder_contains_only_files_with_extension(s3_folder_path4, '.parquet') != True :
    raise ValueError('folder_contains_only_files_with_extension FAILURE')
if s3.folder_contains_only_files_with_extension(s3_folder_path5, 'parquet') != False :
    raise ValueError('folder_contains_only_files_with_extension FAILURE')

print("===> folder_contains_only_files_with_extension ===> OK")  


## =====================> DATATYPE MODULE TESTING <========================= ##

### ### ### ### ### ### ### ### ### ### ###
### translate_metadata_type_to_type ###
### ### ### ### ### ### ### ### ### ### ###
test_types = {
    "character" : {"glue" : "string", "spark": "StringType"},
    "int" : {"glue" : "int", "spark": "IntegerType"},
    "long" : {"glue" : "bigint", "spark": "LongType"},
    "float" : {"glue" : "float", "spark": "FloatType"},
    "double" : {"glue" : "double", "spark": "DoubleType"},
    "date" : {"glue" : "date", "spark": "DateType"},
    "datetime" : {"glue" : "timestamp", "spark": "TimestampType"},
    "boolean" : {"glue" : "boolean", "spark": "BooleanType"}
}
for k in test_types.keys() :
    if datatypes.translate_metadata_type_to_type(k, "glue") != test_types[k]["glue"] :
        raise ValueError("translate_metadata_type_to_type FAILURE")
    if datatypes.translate_metadata_type_to_type(k, "spark") != test_types[k]["spark"] :
        raise ValueError("translate_metadata_type_to_type FAILURE")

print("===> translate_metadata_type_to_type ===> OK")   


### ### ### ### ### ### ### ### ### ### ###
### create_spark_schema_from_metadata ###
### ### ### ### ### ### ### ### ### ### ###
metadata = s3.read_json_from_s3('s3://alpha-gluejobutils/testing/meta_data/diamonds.json')
schema = datatypes.create_spark_schema_from_metadata(metadata)
schema2 = datatypes.create_spark_schema_from_metadata_file('s3://alpha-gluejobutils/testing/meta_data/diamonds.json')
if schema != schema2 :
    raise ValueError("create_spark_schema_from_metadata | create_spark_schema_from_metadata_file FAILURE")
    
for s, m in zip(schema, metadata['columns']) :
    if s.name != m['name'] :
        raise ValueError('create_spark_schema_from_metadata FAILURE')
        
schema3 = datatypes.create_spark_schema_from_metadata(metadata, exclude_cols=['carat'], non_nullable_cols=['x','y','z'])        
if 'carat' in [s.name for s in schema3] :
    raise ValueError('create_spark_schema_from_metadata FAILURE')

    
non_nulls = [s.name for s in schema3 if not s.nullable]
if non_nulls != ['x','y','z'] :
    raise ValueError('create_spark_schema_from_metadata FAILURE')
print("===> create_spark_schema_from_metadata ===> OK")


### ### ### ### ### ### 
### align_df_to_meta ###
### ### ### ### ### ### 
from gluejobutils import s3
from gluejobutils import datatypes

csv_path = 's3://alpha-gluejobutils/testing/data/diamonds_csv/'
meta_path = 's3://alpha-gluejobutils/testing/meta_data/diamonds.json'
meta = s3.read_json_from_s3(meta_path)
df = spark.read.csv(csv_path, header = True)
df2 = datatypes.align_df_to_meta(df, meta)
if df2.count() != df.count() :
    raise ValueError('align_df_to_meta FAILURE') 
for dc, dc2 in zip(df.columns, df2.columns) :
    if dc != dc2 :
        raise ValueError('align_df_to_meta FAILURE')
for s, c in zip(df2.schema, meta['columns']) :
    if s.name != c['name'] :
        raise ValueError('align_df_to_meta FAILURE')
    if str(s.dataType) != datatypes.translate_metadata_type_to_type(c['type'], 'spark') :
        raise ValueError('align_df_to_meta FAILURE')

df3 = datatypes.align_df_to_meta(df, meta, exclude_columns=['x','y','z'])
if any([e in df3.columns for e in ['x','y','z']]) :
    raise ValueError('align_df_to_meta FAILURE')

meta['columns'].append({"name" : "date_test", "type" : 'date', 'description' : ''})
meta['columns'].append({"name" : "datetime_test", "type" : 'datetime', 'description' : ''})

df3 = df3.withColumn('date_test', F.to_date(F.lit("2018-01-01")))
df3 = df3.withColumn('datetime_test', F.to_timestamp(F.lit("2018-01-01 00:00:00")))
try :
    df4 = datatypes.align_df_to_meta(df3, meta)
except ValueError as e :
    if 'ETL_ERROR' not in str(e) :
        raise ValueError('align_df_to_meta FAILURE')
        
df4 = datatypes.align_df_to_meta(df3, meta, null_missing_cols=True)
fr = df4.take(1)[0].asDict()
if fr["diamond_id"] != 0 or fr["x"] is not None or fr["y"] is not None or fr["z"] is not None :
    raise ValueError('align_df_to_meta FAILURE')
print("===> align_df_to_meta ===> OK") 


## =====================> SCD MODULE TESTING <========================= ##

### ### ### ### ### ### ### ### 
###python and java timestamp ###
### ### ### ### ### ### ### ### 
if datetime.datetime.strptime(scd.static_record_end_datetime, scd.standard_datetime_format_python).strftime(scd.standard_datetime_format_python) != scd.static_record_end_datetime :
    raise ValueError("python and java timestamp formats do not match")
print("===> python to java timestamp convertion ===> OK")


### ### ### ### ### ### ### ### 
### dea_record_start_date ###
### ### ### ### ### ### ### ### 
csv_path = 's3://alpha-gluejobutils/testing/data/diamonds_csv/'
meta_path = 's3://alpha-gluejobutils/testing/meta_data/diamonds.json'
start_date = '2018-01-01'
start_datetime = '2018-01-01 01:00:00'
meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df = spark.read.csv(csv_path, header = True, schema = meta)
df = scd.set_dea_record_start_datetime(df, '2018-01-01 01:00:00')

if df.take(1)[0].asDict()['dea_record_start_datetime'].strftime('%Y-%m-%d %H:%M:%S') != start_datetime :
    raise ValueError('set_dea_record_start_date FAILURE')
    
df = scd.set_dea_record_start_datetime(df, '12:34:56 31/12/2017', datetime_format="HH:mm:ss dd/MM/yyyy")
if df.take(1)[0].asDict()['dea_record_start_datetime'].strftime('%Y-%m-%d %H:%M:%S') != '2017-12-31 12:34:56' :
    raise ValueError('set_dea_record_start_datetime FAILURE')

if not isinstance(df.take(1)[0].asDict()['dea_record_start_datetime'], datetime.datetime) :
    raise ValueError('dea_record_start_date FAILURE')
print("===> set_dea_record_start_datetime ===> OK") 


### ### ### ### ### ### ### ### 
### set_dea_record_end_date ###
### ### ### ### ### ### ### ### 
df = scd.set_dea_record_end_datetime(df)
end_datetime = '2999-01-01 00:00:00'
if df.take(1)[0].asDict()['dea_record_end_datetime'].strftime('%Y-%m-%d %H:%M:%S') != end_datetime :
    raise ValueError('dea_record_end_date FAILURE')
if not isinstance(df.take(1)[0].asDict()['dea_record_end_datetime'], datetime.datetime) :
    raise ValueError('dea_record_end_date FAILURE')

df = scd.set_dea_record_end_datetime(df, '2018-01-01 01:00:00')
if df.take(1)[0].asDict()['dea_record_end_datetime'].strftime('%Y-%m-%d %H:%M:%S') != '2018-01-01 01:00:00' :
    raise ValueError('dea_record_end_date FAILURE')
print("===> set_dea_record_end_datetime ===> OK") 


### ### ### ### ### ### ### ### 
### init_dea_record_dates ###
### ### ### ### ### ### ### ### 
start_datetime = '2017-01-01 12:51:43'
end_datetime = '2999-01-01 00:00:00'

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df = spark.read.csv(csv_path, header = True, schema=meta)
df = scd.init_dea_record_datetimes(df, start_datetime)

if df.take(1)[0].asDict()['dea_record_start_datetime'].strftime('%Y-%m-%d %H:%M:%S') != start_datetime :
    raise ValueError('init_dea_record_dates FAILURE')
if not isinstance(df.take(1)[0].asDict()['dea_record_start_datetime'], datetime.datetime) :
    raise ValueError('init_dea_record_dates FAILURE')
if df.take(1)[0].asDict()['dea_record_end_datetime'].strftime('%Y-%m-%d %H:%M:%S') != end_datetime :
    raise ValueError('init_dea_record_dates FAILURE')
if not isinstance(df.take(1)[0].asDict()['dea_record_end_datetime'], datetime.datetime) :
    raise ValueError('init_dea_record_dates FAILURE')
print("===> init_dea_record_datetimes ===> OK")


### ### ### ### ### ### ### ### ### ###
### update_dea_record_end_datetime ###
### ### ### ### ### ### ### ### ### ###
test1_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 1, 23, 45)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 23, 45), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test2_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 1, 23, 45)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 23, 45), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test3_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df_old = spark.read.csv(csv_path, header = True, schema=meta)
df_new = spark.read.csv(csv_path, header = True).filter('diamond_id < 1000')

df_old = scd.init_dea_record_datetimes(df_old, '2018-01-01 01:00:00')
df_new = scd.init_dea_record_datetimes(df_new, '2018-01-01 01:23:45')

df = df_old.union(df_new)
df = scd.update_dea_record_end_datetime(df, 'diamond_id')

if df.count() != 54940 :
    raise ValueError('update_dea_record_end_date FAILURE')
    
df.createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df ORDER BY dea_record_start_datetime, dea_record_end_datetime")
test2 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df WHERE diamond_id < 1000 ORDER BY dea_record_start_datetime, dea_record_end_datetime")
test3 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df WHERE diamond_id >= 1000 ORDER BY dea_record_start_datetime, dea_record_end_datetime")

if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test2.collect()) != sorted(test2_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test3.collect()) != sorted(test3_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
print("===> update_dea_record_end_datetime ===> OK")


### ### ### ### ### ### ### ###
### upsert_table_by_record ###
### ### ### ### ### ### ### ###
bucket = 'alpha-gluejobutils'

db_table_path = 's3://{}/database/table1/'.format(bucket)
db_table_path_tmp = 's3://{}/database/table1_tmp/'.format(bucket)

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df_old = spark.read.csv(csv_path, header = True, schema=meta)
df_old = scd.init_dea_record_datetimes(df_old, '2018-01-01 01:00:00')
df_old.write.mode('overwrite').parquet(db_table_path)

df_new = spark.read.csv(csv_path, header = True, schema=meta).filter('diamond_id < 1000')
df_new = scd.init_dea_record_datetimes(df_new, '2018-01-01 01:23:45')

scd.upsert_table_by_record(spark=spark, new_df=df_new, table_db_base_path=db_table_path, update_by_cols = ['diamond_id'], coalesce_size = 4, update_latest_records_only = True)

if spark.read.parquet(db_table_path_tmp).count() != 54940 :
    raise ValueError('upsert_table_by_record FAILURE')
if spark.read.parquet(db_table_path_tmp+'dea_record_update_type=old/').count() != 52940 :
    raise ValueError('upsert_table_by_record FAILURE')
if spark.read.parquet(db_table_path_tmp+'dea_record_update_type=new/').count() != 2000 :
    raise ValueError('upsert_table_by_record FAILURE')

spark.read.parquet(db_table_path_tmp).createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
test2 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='new'")
test3 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='old'")

if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test2.collect()) != sorted(test2_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test3.collect()) != sorted(test3_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
print("===> upsert_table_by_record () ===> OK")


###Â Test write_update_from_tmp_to_table_db (1/3) ###
scd.write_update_from_tmp_to_table_db(spark, db_table_path, partition_path = '', coalesce_size = 4)
if spark.read.parquet(db_table_path).count() != 54940 :
    raise ValueError('write_update_from_tmp_to_table_db FAILURE')

spark.read.parquet(db_table_path_tmp).createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("write_update_from_tmp_to_table_db FAILURE")
print("===> write_update_from_tmp_to_table_db (1/3) ===> OK")


db_table_path = 's3://{}/database/table2/'.format(bucket)
db_table_path_tmp = 's3://{}/database/table2_tmp/'.format(bucket)

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df_old = spark.read.csv(csv_path, header = True, schema=meta)
df_old = scd.init_dea_record_datetimes(df_old, '2018-01-01 01:00:00')
df_old = df_old.withColumn('partition_bin', F.floor(df_old.diamond_id / 10000).cast(IntegerType()))
df_old.write.mode('overwrite').partitionBy('partition_bin').parquet(db_table_path)

df_new = df_old.filter('diamond_id < 1000')
df_new = scd.init_dea_record_datetimes(df_new, '2018-01-01 01:23:45').drop('partition_bin')

scd.upsert_table_by_record(spark=spark, new_df=df_new, table_db_base_path=db_table_path, update_by_cols=['diamond_id'], partition_path='partition_bin=0')

if spark.read.parquet(db_table_path_tmp).count() != 11000 :
    raise ValueError('upsert_table_by_record FAILURE')
if spark.read.parquet(db_table_path_tmp + '/partition_bin=0/dea_record_update_type=old/').count() != 9000 :
    raise ValueError('upsert_table_by_record FAILURE')
if spark.read.parquet(db_table_path_tmp+'/partition_bin=0/dea_record_update_type=new/').count() != 2000 :
    raise ValueError('upsert_table_by_record FAILURE')

spark.read.parquet(db_table_path_tmp).createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
test2 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='new'")
test3 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='old'")

if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test2.collect()) != sorted(test2_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
if sorted(test3.collect()) != sorted(test3_ans.collect()) :
    raise ValueError("update_dea_record_end_date FAILURE")
print("===> upsert_table_by_record (2/2) ===> OK")


###Â Test write_update_from_tmp_to_table_db (2/3) ###
scd.write_update_from_tmp_to_table_db(spark, db_table_path, partition_path = 'partition_bin=0', coalesce_size = 2)
if spark.read.parquet(db_table_path).count() != 54940 :
    raise ValueError('write_update_from_tmp_to_table_db FAILURE')

spark.read.parquet(db_table_path).createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("write_update_from_tmp_to_table_db FAILURE")
print("===> write_update_from_tmp_to_table_db (2/3) ===> OK")


### ### ### ### ### ### ### ### ### ### ###
### upsert_table_partition_with_new_df ###
### ### ### ### ### ### ### ### ### ### ###
test1_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 2, 0)), 
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 2, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test2_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 2, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test3_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 2, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

db_table_path = 's3://{}/database/table3/'.format(bucket)
db_table_path_tmp = 's3://{}/database/table3_tmp/'.format(bucket)

meta = datatypes.create_spark_schema_from_metadata_file(meta_path)
df_old = spark.read.csv(csv_path, header = True, schema=meta)
df_old = scd.init_dea_record_datetimes(df_old, '2018-01-01 01:00:00')
df_old = df_old.withColumn('partition_bin', F.floor(df_old.diamond_id / 10000).cast(IntegerType()))
df_old.write.mode('overwrite').partitionBy('partition_bin').parquet(db_table_path)

df_new = df_old.filter('partition_bin = 0')
df_new = scd.init_dea_record_datetimes(df_new, '2018-01-01 02:00:00')

scd.upsert_table_partition_with_new_df(spark, new_df=df_new, table_db_base_path = db_table_path, partition_path='partition_bin=0')

spark.read.parquet(db_table_path_tmp).createOrReplaceTempView('df')
test1 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
test2 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='new'")
test3 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where dea_record_update_type='old'")

if sorted(test1.collect()) != sorted(test1_ans.collect()) :
    raise ValueError("upsert_table_partition_with_new_df FAILURE")
if sorted(test2.collect()) != sorted(test2_ans.collect()) :
    raise ValueError("upsert_table_partition_with_new_df FAILURE")
if sorted(test3.collect()) != sorted(test3_ans.collect()) :
    raise ValueError("upsert_table_partition_with_new_df FAILURE")
print("===> upsert_table_partition_with_new_df ===> OK")


###Â Test write_update_from_tmp_to_table_db (3/3) ###
test4_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 2, 0)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 2, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test5_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2018, 1, 1, 2, 0)),
                                   Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 2, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

test6_ans = spark.createDataFrame([Row(dea_record_start_datetime=datetime.datetime(2018, 1, 1, 1, 0), dea_record_end_datetime=datetime.datetime(2999, 1, 1, 0, 0))]).select('dea_record_start_datetime', 'dea_record_end_datetime')

scd.write_update_from_tmp_to_table_db(spark, db_table_path, partition_path = 'partition_bin=0/', coalesce_size = 4)
if spark.read.parquet(db_table_path).count() != 63940 :
    raise ValueError('write_update_from_tmp_to_table_db FAILURE')

spark.read.parquet(db_table_path).createOrReplaceTempView('df')
test4 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df")
if sorted(test4.collect()) != sorted(test4_ans.collect()) :
    raise ValueError("write_update_from_tmp_to_table_db FAILURE")
    
test5 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where partition_bin = 0")
if sorted(test5.collect()) != sorted(test5_ans.collect()) :
    raise ValueError("write_update_from_tmp_to_table_db FAILURE")
    
test6 = spark.sql("SELECT DISTINCT dea_record_start_datetime, dea_record_end_datetime FROM df where partition_bin <> 0")
if sorted(test6.collect()) != sorted(test6_ans.collect()) :
    raise ValueError("write_update_from_tmp_to_table_db FAILURE")
print("===> write_update_from_tmp_to_table_db (3/3) ===> OK")

job.commit()